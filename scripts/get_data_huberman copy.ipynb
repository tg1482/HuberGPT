{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanmaygupta/miniconda3/envs/huberbot_env/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,re\n",
    "import yt_dlp\n",
    "import json\n",
    "import time\n",
    "import math \n",
    "import httplib2\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from youtubesearchpython import *\n",
    "from langchain.llms import OpenAIChat\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': [{'type': 'channel', 'id': 'UC2D2CMWXMOVWx7giW1n3LIg', 'title': 'Andrew Huberman', 'thumbnails': [{'url': '//yt3.ggpht.com/5ONImZvpa9_hYK12Xek2E2JLzRc732DWsZMX2F-AZ1cTutTQLBuAmcEtFwrCgypqJncl5HrV2w=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/5ONImZvpa9_hYK12Xek2E2JLzRc732DWsZMX2F-AZ1cTutTQLBuAmcEtFwrCgypqJncl5HrV2w=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'Welcome to the official '}, {'text': 'Huberman', 'bold': True}, {'text': ' Lab YouTube channel. The '}, {'text': 'Huberman', 'bold': True}, {'text': ' Lab podcast is hosted by Dr. Andrew '}, {'text': 'Huberman', 'bold': True}, {'text': ',\\xa0...'}], 'subscribers': '@hubermanlab', 'link': 'https://www.youtube.com/channel/UC2D2CMWXMOVWx7giW1n3LIg'}, {'type': 'channel', 'id': 'UCkZjTZNvuxq1CYMS3cwZa1Q', 'title': 'Huberman Lab Clips', 'thumbnails': [{'url': '//yt3.ggpht.com/RpRPtZhc7dokY3Bkw-yg9kIr9G7_SxPuL_F-YouM6BT4B4h7fPxAY8sS3juhbO5ckTM3g8tIKg=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/RpRPtZhc7dokY3Bkw-yg9kIr9G7_SxPuL_F-YouM6BT4B4h7fPxAY8sS3juhbO5ckTM3g8tIKg=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'Welcome to the official '}, {'text': 'Huberman', 'bold': True}, {'text': \" Lab Clips YouTube channel. Here you'll find clips from the \"}, {'text': 'Huberman', 'bold': True}, {'text': ' Lab podcast. For full\\xa0...'}], 'subscribers': '@HubermanLabClips', 'link': 'https://www.youtube.com/channel/UCkZjTZNvuxq1CYMS3cwZa1Q'}, {'type': 'channel', 'id': 'UCCYP_dEoB-ZElhpQ4JvlwCw', 'title': 'Huberman HUB', 'thumbnails': [{'url': '//yt3.ggpht.com/aFWw12USizGa9yVK3hpCacdvwgusp8HEPC7dpIJoHvCtwEiy_nZAAeioYnEigB1GO7IpsHwQ6Q=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/aFWw12USizGa9yVK3hpCacdvwgusp8HEPC7dpIJoHvCtwEiy_nZAAeioYnEigB1GO7IpsHwQ6Q=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'About '}, {'text': 'Huberman', 'bold': True}, {'text': ' HUB Science-Based tips for self improvement Produced by '}, {'text': 'Huberman', 'bold': True}, {'text': ' HUB Subscribe now for more\\xa0...'}], 'subscribers': '@HubermanHUB', 'link': 'https://www.youtube.com/channel/UCCYP_dEoB-ZElhpQ4JvlwCw'}, {'type': 'channel', 'id': 'UCCV8cVGLHv9HwvuuonNWAqg', 'title': 'Huberman Highlights', 'thumbnails': [{'url': '//yt3.googleusercontent.com/ytc/AL5GRJXF2coE3b-qoC4JioeRR_KSsb7ZFc9XX8vl7x7MJw=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.googleusercontent.com/ytc/AL5GRJXF2coE3b-qoC4JioeRR_KSsb7ZFc9XX8vl7x7MJw=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': None, 'subscribers': '@HubermanHighlights', 'link': 'https://www.youtube.com/channel/UCCV8cVGLHv9HwvuuonNWAqg'}, {'type': 'channel', 'id': 'UCv3ct6JC-v23M8Kk8qGXDoQ', 'title': 'Virtusan App', 'thumbnails': [{'url': '//yt3.ggpht.com/qynPreO2CwRehWF5Ig31lNpn69Yc1gSG3918ZVRmAu6-PLtNWL0UgNG1xX-Jg8ZXArGsSued=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/qynPreO2CwRehWF5Ig31lNpn69Yc1gSG3918ZVRmAu6-PLtNWL0UgNG1xX-Jg8ZXArGsSued=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'Welcome to the Virtusan YouTube channel. Download our app today from the App Store or Google Play to get started with\\xa0...'}], 'subscribers': '@virtusan', 'link': 'https://www.youtube.com/channel/UCv3ct6JC-v23M8Kk8qGXDoQ'}, {'type': 'channel', 'id': 'UCLN9NsohBMlfna7mIeem9JA', 'title': 'Terrie Huberman', 'thumbnails': [{'url': '//yt3.ggpht.com/7WpnVUnJ4m9pPNtUuZeqhUzAJpVXDdsUrW3mex5STYyqr2Wvs4HdVBu8Qwldc6H1elmUHoN-bA=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/7WpnVUnJ4m9pPNtUuZeqhUzAJpVXDdsUrW3mex5STYyqr2Wvs4HdVBu8Qwldc6H1elmUHoN-bA=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'Helping spiritual empaths with anxiety feel more control in their lives! I help you CLEAR anxiety from the past, ALIGN your energy\\xa0...'}], 'subscribers': '@TerrieH', 'link': 'https://www.youtube.com/channel/UCLN9NsohBMlfna7mIeem9JA'}, {'type': 'channel', 'id': 'UC8V941FJNsVzd2zkGc2wezg', 'title': 'AndrewðŸ§¬Huberman ', 'thumbnails': [{'url': '//yt3.ggpht.com/FITrJRAO4kokrJkcRNlvMkl-ynhmkyqanQMhJS00Z_qt0RuUUdGfPXmGtoT5Ii-CwdxXv8inpA=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/FITrJRAO4kokrJkcRNlvMkl-ynhmkyqanQMhJS00Z_qt0RuUUdGfPXmGtoT5Ii-CwdxXv8inpA=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': \"It's not a normal YouTube channel. It's a cult of young men who are fed up with hypocrisy and the nonsense of a degenerate life\\xa0...\"}], 'subscribers': '@hubermanandrew', 'link': 'https://www.youtube.com/channel/UC8V941FJNsVzd2zkGc2wezg'}, {'type': 'channel', 'id': 'UCqksyQ6Uv_Vk4UQniiGkIlQ', 'title': 'Andrew Huberman Wisdom', 'thumbnails': [{'url': '//yt3.ggpht.com/sYqlQb9rzj5iz13jZOlerNJnS2te4rJQQoXg1k0ew6BENPD98o-jC5Qrclcy58C2h35M5gq8vdk=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.ggpht.com/sYqlQb9rzj5iz13jZOlerNJnS2te4rJQQoXg1k0ew6BENPD98o-jC5Qrclcy58C2h35M5gq8vdk=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': [{'text': 'Andrew '}, {'text': 'Huberman', 'bold': True}, {'text': ' Wisdom Neuroscience | Andrew '}, {'text': 'Huberman', 'bold': True}, {'text': ' Shorts | Good Habits | Lifestyle Tips EVERYDAY TIPS to\\xa0...'}], 'subscribers': '@andrewhubermanwisdom', 'link': 'https://www.youtube.com/channel/UCqksyQ6Uv_Vk4UQniiGkIlQ'}, {'type': 'channel', 'id': 'UCbgQGBIb4LHJLPTKamTJfNw', 'title': 'Andrew Huberman', 'thumbnails': [{'url': '//yt3.googleusercontent.com/ytc/AL5GRJX5zcrQNsA4qfFfU-QGkuyqlY1TqU6_eS_JJg=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.googleusercontent.com/ytc/AL5GRJX5zcrQNsA4qfFfU-QGkuyqlY1TqU6_eS_JJg=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': None, 'subscribers': '@Andrew_Huberman91', 'link': 'https://www.youtube.com/channel/UCbgQGBIb4LHJLPTKamTJfNw'}, {'type': 'channel', 'id': 'UC4LWz0Pi5OsCpCRrV0RKeAw', 'title': 'Elisa Huberman', 'thumbnails': [{'url': '//yt3.googleusercontent.com/ytc/AL5GRJVhs0NrAcLV4onIwq-4klbK_uEpY1wPf1y96eVx=s88-c-k-c0x00ffffff-no-rj-mo', 'width': 88, 'height': 88}, {'url': '//yt3.googleusercontent.com/ytc/AL5GRJVhs0NrAcLV4onIwq-4klbK_uEpY1wPf1y96eVx=s176-c-k-c0x00ffffff-no-rj-mo', 'width': 176, 'height': 176}], 'videoCount': None, 'descriptionSnippet': None, 'subscribers': '@elisahuberman3408', 'link': 'https://www.youtube.com/channel/UC4LWz0Pi5OsCpCRrV0RKeAw'}]}\n"
     ]
    }
   ],
   "source": [
    "from youtubesearchpython import ChannelsSearch\n",
    "\n",
    "channelsSearch = ChannelsSearch('huberman', limit = 10, region = 'US')\n",
    "\n",
    "print(channelsSearch.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lext GPT\n",
    "\n",
    "`Here, we will prepare the VectorDB index for Lex Fridman podcast:`\n",
    "\n",
    "* Scrape source data from: https://karpathy.ai/lexicap/\n",
    "* Use Whisper to transcribe episodes that Karpathy has not already done\n",
    "* Chunk data\n",
    "* Embed it to Pinecone\n",
    "* Test VectorDBQA chain on it \n",
    "* App (https://lex-gpt.vercel.app/) will read from same Pinecone DB\n",
    " \n",
    "`1. Get video urls -` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtubesearchpython import Playlist, playlist_from_channel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "playlist_from_channel_id() got an unexpected keyword argument 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Videos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m channel_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUC2D2CMWXMOVWx7giW1n3LIg\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# Get ID from ChannelsSearch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m playlist \u001b[39m=\u001b[39m Playlist(playlist_from_channel_id(channel_id, limit\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# Episode data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m stor_metadata\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n",
      "\u001b[0;31mTypeError\u001b[0m: playlist_from_channel_id() got an unexpected keyword argument 'limit'"
     ]
    }
   ],
   "source": [
    "# Videos\n",
    "channel_id = \"UC2D2CMWXMOVWx7giW1n3LIg\" # Get ID from ChannelsSearch\n",
    "playlist = Playlist(playlist_from_channel_id(channel_id))\n",
    "\n",
    "# Episode data\n",
    "stor_metadata=pd.DataFrame()\n",
    "for i, v in enumerate(playlist.videos):\n",
    "    try:\n",
    "        ep_number = int(i + 1)\n",
    "        stor_metadata.loc[v['title'],'number']=ep_number\n",
    "        stor_metadata.loc[v['title'],'link']=v['link']\n",
    "        stor_metadata.loc[v['title'],'title']=v['title']\n",
    "        stor_metadata.loc[v['title'],'img']=v['thumbnails'][3]['url']\n",
    "    except Exception as e:\n",
    "        print(\"Failed on %s\", v['title'])\n",
    "        print(e)\n",
    "\n",
    "last_complete_video = 0\n",
    "new_ep = stor_metadata[stor_metadata.number > last_complete_video]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. Get audio -` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 1\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=ulHrUVV3Kq4&list=UU2D2CMWXMOVWx7giW1n3LIg&index=1&pp=iAQB\n",
      "[youtube:tab] Downloading just the video ulHrUVV3Kq4 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ulHrUVV3Kq4\n",
      "[youtube] ulHrUVV3Kq4: Downloading webpage\n",
      "[youtube] ulHrUVV3Kq4: Downloading android player API JSON\n",
      "[info] ulHrUVV3Kq4: Downloading 1 format(s): 140\n",
      "[download] audio/1.m4a has already been downloaded\n",
      "[download] 100% of  110.03MiB\n",
      "[ExtractAudio] Not converting audio audio/1.m4a; file is already in target format m4a\n",
      "EPISODE: 2\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=K-TW2Chpz4k&list=UU2D2CMWXMOVWx7giW1n3LIg&index=2&pp=iAQB\n",
      "[youtube:tab] Downloading just the video K-TW2Chpz4k because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=K-TW2Chpz4k\n",
      "[youtube] K-TW2Chpz4k: Downloading webpage\n",
      "[youtube] K-TW2Chpz4k: Downloading android player API JSON\n",
      "[info] K-TW2Chpz4k: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 12\n",
      "[download] Destination: audio/2.m4a\n",
      "[download] 100% of  110.03MiB in 00:00:06 at 16.62MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/2.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/2.m4a; file is already in target format m4a\n",
      "EPISODE: 3\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=cp9GXl9Qk_s&list=UU2D2CMWXMOVWx7giW1n3LIg&index=3&pp=iAQB\n",
      "[youtube:tab] Downloading just the video cp9GXl9Qk_s because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=cp9GXl9Qk_s\n",
      "[youtube] cp9GXl9Qk_s: Downloading webpage\n",
      "[youtube] cp9GXl9Qk_s: Downloading android player API JSON\n",
      "[info] cp9GXl9Qk_s: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 3\n",
      "[download] Destination: audio/3.m4a\n",
      "[download] 100% of   21.71MiB in 00:00:01 at 16.96MiB/s              \n",
      "[FixupM4a] Correcting container of \"audio/3.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/3.m4a; file is already in target format m4a\n",
      "EPISODE: 4\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=ufsIA5NARIo&list=UU2D2CMWXMOVWx7giW1n3LIg&index=4&pp=iAQB\n",
      "[youtube:tab] Downloading just the video ufsIA5NARIo because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ufsIA5NARIo\n",
      "[youtube] ufsIA5NARIo: Downloading webpage\n",
      "[youtube] ufsIA5NARIo: Downloading android player API JSON\n",
      "[info] ufsIA5NARIo: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 20\n",
      "[download] Destination: audio/4.m4a\n",
      "[download] 100% of  194.40MiB in 00:00:10 at 17.90MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/4.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/4.m4a; file is already in target format m4a\n",
      "EPISODE: 5\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=7R3-3HR6-u4&list=UU2D2CMWXMOVWx7giW1n3LIg&index=5&pp=iAQB\n",
      "[youtube:tab] Downloading just the video 7R3-3HR6-u4 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=7R3-3HR6-u4\n",
      "[youtube] 7R3-3HR6-u4: Downloading webpage\n",
      "[youtube] 7R3-3HR6-u4: Downloading android player API JSON\n",
      "[info] 7R3-3HR6-u4: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 16\n",
      "[download] Destination: audio/5.m4a\n",
      "[download] 100% of  156.60MiB in 00:00:07 at 21.74MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/5.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/5.m4a; file is already in target format m4a\n",
      "EPISODE: 6\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=at37Y8rKDlA&list=UU2D2CMWXMOVWx7giW1n3LIg&index=6&pp=iAQB\n",
      "[youtube:tab] Downloading just the video at37Y8rKDlA because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=at37Y8rKDlA\n",
      "[youtube] at37Y8rKDlA: Downloading webpage\n",
      "[youtube] at37Y8rKDlA: Downloading android player API JSON\n",
      "[info] at37Y8rKDlA: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 14\n",
      "[download] Destination: audio/6.m4a\n",
      "[download] 100% of  131.85MiB in 00:00:05 at 22.15MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/6.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/6.m4a; file is already in target format m4a\n",
      "EPISODE: 7\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=CDUetQMKM6g&list=UU2D2CMWXMOVWx7giW1n3LIg&index=7&pp=iAQB\n",
      "[youtube:tab] Downloading just the video CDUetQMKM6g because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=CDUetQMKM6g\n",
      "[youtube] CDUetQMKM6g: Downloading webpage\n",
      "[youtube] CDUetQMKM6g: Downloading android player API JSON\n",
      "[info] CDUetQMKM6g: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 15\n",
      "[download] Destination: audio/7.m4a\n",
      "[download] 100% of  140.77MiB in 00:00:06 at 22.55MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/7.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/7.m4a; file is already in target format m4a\n",
      "EPISODE: 8\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=S8nPJU9xkNw&list=UU2D2CMWXMOVWx7giW1n3LIg&index=8&pp=iAQB\n",
      "[youtube:tab] Downloading just the video S8nPJU9xkNw because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=S8nPJU9xkNw\n",
      "[youtube] S8nPJU9xkNw: Downloading webpage\n",
      "[youtube] S8nPJU9xkNw: Downloading android player API JSON\n",
      "[info] S8nPJU9xkNw: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 3\n",
      "[download] Destination: audio/8.m4a\n",
      "[download] 100% of   21.51MiB in 00:00:02 at 9.06MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/8.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/8.m4a; file is already in target format m4a\n",
      "EPISODE: 9\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=q37ARYnRDGc&list=UU2D2CMWXMOVWx7giW1n3LIg&index=9&pp=iAQB\n",
      "[youtube:tab] Downloading just the video q37ARYnRDGc because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=q37ARYnRDGc\n",
      "[youtube] q37ARYnRDGc: Downloading webpage\n",
      "[youtube] q37ARYnRDGc: Downloading android player API JSON\n",
      "[info] q37ARYnRDGc: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 18\n",
      "[download] Destination: audio/9.m4a\n",
      "[download] 100% of  171.83MiB in 00:00:09 at 18.99MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/9.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/9.m4a; file is already in target format m4a\n",
      "EPISODE: 10\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=x4m_PdFbu-s&list=UU2D2CMWXMOVWx7giW1n3LIg&index=10&pp=iAQB\n",
      "[youtube:tab] Downloading just the video x4m_PdFbu-s because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=x4m_PdFbu-s\n",
      "[youtube] x4m_PdFbu-s: Downloading webpage\n",
      "[youtube] x4m_PdFbu-s: Downloading android player API JSON\n",
      "[info] x4m_PdFbu-s: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 13\n",
      "[download] Destination: audio/10.m4a\n",
      "[download] 100% of  128.56MiB in 00:00:07 at 17.88MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/10.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/10.m4a; file is already in target format m4a\n",
      "EPISODE: 11\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=juD99_sPWGU&list=UU2D2CMWXMOVWx7giW1n3LIg&index=11&pp=iAQB\n",
      "[youtube:tab] Downloading just the video juD99_sPWGU because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=juD99_sPWGU\n",
      "[youtube] juD99_sPWGU: Downloading webpage\n",
      "[youtube] juD99_sPWGU: Downloading android player API JSON\n",
      "[info] juD99_sPWGU: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 18\n",
      "[download] Destination: audio/11.m4a\n",
      "[download] 100% of  171.40MiB in 00:00:09 at 17.56MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/11.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/11.m4a; file is already in target format m4a\n",
      "EPISODE: 12\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=BMTt8gSl13s&list=UU2D2CMWXMOVWx7giW1n3LIg&index=12&pp=iAQB\n",
      "[youtube:tab] Downloading just the video BMTt8gSl13s because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=BMTt8gSl13s\n",
      "[youtube] BMTt8gSl13s: Downloading webpage\n",
      "[youtube] BMTt8gSl13s: Downloading android player API JSON\n",
      "[info] BMTt8gSl13s: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 12\n",
      "[download] Destination: audio/12.m4a\n",
      "[download] 100% of  116.38MiB in 00:00:05 at 22.33MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/12.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/12.m4a; file is already in target format m4a\n",
      "EPISODE: 13\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=UIy-WQCZd4M&list=UU2D2CMWXMOVWx7giW1n3LIg&index=13&pp=iAQB\n",
      "[youtube:tab] Downloading just the video UIy-WQCZd4M because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=UIy-WQCZd4M\n",
      "[youtube] UIy-WQCZd4M: Downloading webpage\n",
      "[youtube] UIy-WQCZd4M: Downloading android player API JSON\n",
      "[info] UIy-WQCZd4M: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 18\n",
      "[download] Destination: audio/13.m4a\n",
      "[download] 100% of  172.36MiB in 00:00:07 at 23.21MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/13.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/13.m4a; file is already in target format m4a\n",
      "EPISODE: 14\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=CGjdgy0cwGk&list=UU2D2CMWXMOVWx7giW1n3LIg&index=14&pp=iAQB\n",
      "[youtube:tab] Downloading just the video CGjdgy0cwGk because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=CGjdgy0cwGk\n",
      "[youtube] CGjdgy0cwGk: Downloading webpage\n",
      "[youtube] CGjdgy0cwGk: Downloading android player API JSON\n",
      "[info] CGjdgy0cwGk: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 14\n",
      "[download] Destination: audio/14.m4a\n",
      "[download] 100% of  135.88MiB in 00:00:06 at 20.73MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/14.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/14.m4a; file is already in target format m4a\n",
      "EPISODE: 15\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=oNkDA2F7CjM&list=UU2D2CMWXMOVWx7giW1n3LIg&index=15&pp=iAQB\n",
      "[youtube:tab] Downloading just the video oNkDA2F7CjM because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=oNkDA2F7CjM\n",
      "[youtube] oNkDA2F7CjM: Downloading webpage\n",
      "[youtube] oNkDA2F7CjM: Downloading android player API JSON\n",
      "[info] oNkDA2F7CjM: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 22\n",
      "[download] Destination: audio/15.m4a\n",
      "[download] 100% of  211.95MiB in 00:00:09 at 22.05MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/15.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/15.m4a; file is already in target format m4a\n",
      "EPISODE: 16\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=GVRDGQhoEYQ&list=UU2D2CMWXMOVWx7giW1n3LIg&index=16&pp=iAQB\n",
      "[youtube:tab] Downloading just the video GVRDGQhoEYQ because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=GVRDGQhoEYQ\n",
      "[youtube] GVRDGQhoEYQ: Downloading webpage\n",
      "[youtube] GVRDGQhoEYQ: Downloading android player API JSON\n",
      "[info] GVRDGQhoEYQ: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 15\n",
      "[download] Destination: audio/16.m4a\n",
      "[download] 100% of  143.61MiB in 00:00:05 at 24.28MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/16.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/16.m4a; file is already in target format m4a\n",
      "EPISODE: 17\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=CyDLbrZK75U&list=UU2D2CMWXMOVWx7giW1n3LIg&index=17&pp=iAQB\n",
      "[youtube:tab] Downloading just the video CyDLbrZK75U because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=CyDLbrZK75U\n",
      "[youtube] CyDLbrZK75U: Downloading webpage\n",
      "[youtube] CyDLbrZK75U: Downloading android player API JSON\n",
      "[info] CyDLbrZK75U: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 26\n",
      "[download] Destination: audio/17.m4a\n",
      "[download] 100% of  258.73MiB in 00:00:11 at 22.50MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/17.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/17.m4a; file is already in target format m4a\n",
      "EPISODE: 18\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=O1YRwWmue4Y&list=UU2D2CMWXMOVWx7giW1n3LIg&index=18&pp=iAQB\n",
      "[youtube:tab] Downloading just the video O1YRwWmue4Y because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=O1YRwWmue4Y\n",
      "[youtube] O1YRwWmue4Y: Downloading webpage\n",
      "[youtube] O1YRwWmue4Y: Downloading android player API JSON\n",
      "[info] O1YRwWmue4Y: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 25\n",
      "[download] Destination: audio/18.m4a\n",
      "[download] 100% of  242.85MiB in 00:00:10 at 23.77MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/18.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/18.m4a; file is already in target format m4a\n",
      "EPISODE: 19\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=zEYE-vcVKy8&list=UU2D2CMWXMOVWx7giW1n3LIg&index=19&pp=iAQB\n",
      "[youtube:tab] Downloading just the video zEYE-vcVKy8 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=zEYE-vcVKy8\n",
      "[youtube] zEYE-vcVKy8: Downloading webpage\n",
      "[youtube] zEYE-vcVKy8: Downloading android player API JSON\n",
      "[youtube] zEYE-vcVKy8: Downloading player fa7eb95c\n",
      "[info] zEYE-vcVKy8: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 12\n",
      "[download] Destination: audio/19.m4a\n",
      "[download] 100% of  112.32MiB in 00:00:04 at 23.26MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/19.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/19.m4a; file is already in target format m4a\n",
      "EPISODE: 20\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=ycOBZZeVeAc&list=UU2D2CMWXMOVWx7giW1n3LIg&index=20&pp=iAQB\n",
      "[youtube:tab] Downloading just the video ycOBZZeVeAc because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ycOBZZeVeAc\n",
      "[youtube] ycOBZZeVeAc: Downloading webpage\n",
      "[youtube] ycOBZZeVeAc: Downloading android player API JSON\n",
      "[info] ycOBZZeVeAc: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 17\n",
      "[download] Destination: audio/20.m4a\n",
      "[download] 100% of  167.28MiB in 00:00:06 at 24.74MiB/s                  \n",
      "[FixupM4a] Correcting container of \"audio/20.m4a\"\n",
      "[ExtractAudio] Not converting audio audio/20.m4a; file is already in target format m4a\n",
      "EPISODE: 21\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=uak_dXHh6s4&list=UU2D2CMWXMOVWx7giW1n3LIg&index=21&pp=iAQB\n",
      "[youtube:tab] Downloading just the video uak_dXHh6s4 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=uak_dXHh6s4\n",
      "[youtube] uak_dXHh6s4: Downloading webpage\n",
      "[youtube] uak_dXHh6s4: Downloading android player API JSON\n",
      "[info] uak_dXHh6s4: Downloading 1 format(s): 140\n",
      "[dashsegments] Total fragments: 3\n",
      "[download] Destination: audio/21.m4a\n",
      "[download]  80.9% of ~  29.65MiB at   13.60MiB/s ETA 00:00 (frag 2/3)   "
     ]
    }
   ],
   "source": [
    "# Iterate through episodes \n",
    "for ix in new_ep.index:\n",
    "    \n",
    "    ep_number=int(new_ep.loc[ix,'number'])\n",
    "    print(\"EPISODE: %s\"%ep_number)\n",
    "    img_url=new_ep.loc[ix,'img']\n",
    "    ep_link=new_ep.loc[ix,'link']\n",
    "    # Write img \n",
    "    with open(\"../public/0%s.jpg\"%str(ep_number), 'wb') as f:\n",
    "        response = requests.get(img_url)\n",
    "        f.write(response.content)\n",
    "    # Write audio\n",
    "    ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'outtmpl': 'audio/%s.m4a'%str(ep_number),\n",
    "    'noplaylist': True,\n",
    "    'postprocessors': [{  \n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }]}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download(ep_link)\n",
    "        \n",
    "new_ep.reset_index().to_csv(\"audio_transcription/episodes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. Run Whisper -`\n",
    " \n",
    "* On GPU, ideally: 10-20 min / video on 2080Ti with `medium` model\n",
    "* Run `python run_whisper.py`\n",
    "\n",
    "If running this step on a remote machine:\n",
    "* scp the transcription: `audio_transcription/episodes.csv`\n",
    "* scp the audio files: `audio/*`\n",
    "* Run `python run_whisper.py`\n",
    "* Then, scp the `audio_transcription/` back to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python run_whisper.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4. Scrape Karpathy transcriptions -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text -\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    t=(text_from_html(html))\n",
    "    title=t.split(\"|\")[0].split(\"back to index\")[1].strip()\n",
    "    return t, title\n",
    "\n",
    "# Get links -\n",
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = [l for l in links if \"https\" in l]\n",
    "    return links_clean\n",
    "\n",
    "# Get image -\n",
    "def get_img(URL,title,episode_id):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        imgpath=\"../public/0%s.jpg\"%episode_id\n",
    "        with open(imgpath, 'wb') as f:\n",
    "            if 'http' not in url:\n",
    "                url = '{}{}'.format(site, url)\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "    return imgpath\n",
    "\n",
    "# Full pipeline - \n",
    "def pre_process(URL,episode_id):\n",
    "\n",
    "    t,title=get_text_and_title(URL)\n",
    "    links=get_links(URL)\n",
    "    img=get_img(URL,title,episode_id)\n",
    "    stor_chunk = pd.DataFrame()\n",
    "    stor_chunk['chunks']= t.split(\"link |\")\n",
    "    stor_chunk['clean_chunks']=stor_chunk['chunks'].apply(lambda x: re.sub(r\"[^a-zA-Z ]+\", '', x)).apply(lambda x: x.strip())\n",
    "    stor_chunk['links']=links\n",
    "    all_text = stor_chunk['clean_chunks'].str.cat(sep=' ')\n",
    "    return all_text, links, title\n",
    "\n",
    "# Make splits - \n",
    "def make_splits(chunks,URL):\n",
    "\n",
    "    # ID\n",
    "    episode_id=URL.split(\"/\")[-1].split(\"-\")[0]\n",
    "\n",
    "    # Pre-processing\n",
    "    texts,links,title=pre_process(URL,episode_id)\n",
    "    \n",
    "    # Splits \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=50) \n",
    "    texts_recusive = text_splitter.split_text(texts)\n",
    "    print(len(texts_recusive)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(texts_recusive) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas))\n",
    "    return texts_recusive,metadatas,title,episode_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all pages \n",
    "http = httplib2.Http()\n",
    "status, response = http.request(\"https://karpathy.ai/lexicap/\")\n",
    "links = []\n",
    "for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "    if link.has_attr('href'):\n",
    "        links.append(link['href'])\n",
    "links_tx = [\"https://karpathy.ai/lexicap/\"+l for l in links if \"0\" in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "splits_scrape = [ ]\n",
    "metadatas_scrape = [ ]\n",
    " \n",
    "# Iterate \n",
    "stor=pd.DataFrame()\n",
    "for page in links_tx:\n",
    "    try:\n",
    "        print(\"Writing: %s\"%page)\n",
    "        # Make splits\n",
    "        splits,metadatas,title,episode_id=make_splits(chunks,page)\n",
    "        stor.loc[episode_id,'title']=title \n",
    "        with open('docs/%s.txt'%episode_id, \"w\") as f:\n",
    "            for string in splits:\n",
    "                f.write(string + \"\\n\") \n",
    "        f.close()\n",
    "        with open('metadatas/%s.json'%episode_id, \"w\") as f:\n",
    "            json.dump(metadatas, f)\n",
    "        f.close()\n",
    "        splits_scrape.append(splits)\n",
    "        metadatas_scrape.append(metadatas)\n",
    "    except:\n",
    "        print(\"Error on page: %s\"%page)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5. Get newer transcripts -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "splits_new = [ ]\n",
    "metadatas_new = [ ]\n",
    "\n",
    "# Read the csv file\n",
    "new_ep=pd.read_csv(\"audio_transcription/episodes.csv\",index_col=None)\n",
    "\n",
    "for ix in new_ep.index:\n",
    "\n",
    "    # Get data\n",
    "    title=new_ep.loc[ix,'title']\n",
    "    ep_number=int(new_ep.loc[ix,'number'])\n",
    "    \n",
    "    # Consistency w/ convention used in Karpathy transcription\n",
    "    episode_id=\"0\"+str(ep_number) \n",
    "    file_path='audio_transcription/%s.txt'%str(episode_id)\n",
    "    transcript=pd.read_csv(file_path,sep='\\t',header=None)\n",
    "    transcript.columns=['links','time','chunks']\n",
    "    \n",
    "    # Clean text chunks \n",
    "    transcript['clean_chunks']=transcript['chunks'].astype(str).apply(lambda x: x.strip())\n",
    "    links = list(transcript['links'])\n",
    "    texts = transcript['clean_chunks'].str.cat(sep=' ')\n",
    "\n",
    "    # Splits \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=50) \n",
    "    splits = text_splitter.split_text(texts)\n",
    "    print(len(splits)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(splits) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    \n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas)) \n",
    "\n",
    "    # Append to output \n",
    "    splits_new.append(splits)\n",
    "    metadatas_new.append(metadatas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6. Assemble final list -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of lists \n",
    "splits_all = []\n",
    "# For the initial write \n",
    "# for sublist in [splits_scrape+splits_new]:\n",
    "# For updates -- \n",
    "for sublist in splits_new:\n",
    "    splits_all.extend(sublist)\n",
    "\n",
    "metadatas_all = []\n",
    "# For the initial write \n",
    "# for sublist in [metadatas_scrape+metadatas_new]:\n",
    "# For updates -- \n",
    "for sublist in metadatas_new:\n",
    "    metadatas_all.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7. Embed full dataset in Pinecone VectorDB -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "\n",
    "# Initialize with small set of data - \n",
    "# p = Pinecone.from_texts(splits_all[0:2], \n",
    "#                        embeddings, \n",
    "#                        index_name=index_name, \n",
    "#                        metadatas=metadatas_all[0:2])\n",
    "\n",
    "# Update - \n",
    "index_name = \"lex-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 0\n",
    "num_chunks = math.ceil(len(splits_all) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`8. Read in VectorDB for testing` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`9. Run VectorDBQAWithSourcesChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectordb_sources_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQAWithSourcesChain.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "q = \"What does Eleazar Yukowski think about AI alignment?\"\n",
    "run_vectordb_sources_chain(llm,q,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
